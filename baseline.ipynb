{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5389eb61-a0fb-45e3-b8da-64a2cc5bc7a5",
   "metadata": {
    "execution_count": 1,
    "msg_id": "929a2d3e-3239-4697-9d36-f01b161a1dca"
   },
   "outputs": [],
   "source": "# 导入数据集 空间转录组细胞数据分析初赛数据集\nimport wfio\nimport pandas as pd\n\npd.set_option('display.max_rows', 20)\n_INPUT = '{\"type\":15,\"uri\":\"sample_data/85/task-85-r2wzclmecm\"}'\n# 读取并返回对应的Dataframe\n# 参数as_spark: 为True返回Spark DataFrame，为False返回Pandas DataFrame，默认为False\n    \ndf = wfio.read_dataframe(_INPUT,as_spark = False)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59839535-f38f-4a08-b063-6def49319940",
   "metadata": {
    "execution_count": 2,
    "msg_id": "1d8e577f-61d9-47bf-9189-03eb8b8a07b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/ma-user/anaconda3/envs/python-3.7.10/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
    }
   ],
   "source": "import os\nimport time\nimport anndata\nimport argparse\nimport os.path as osp\nimport pandas as pd\nimport warnings\nimport numpy as np\nimport scanpy as sc\nimport scipy.sparse as sp\nimport torch\nimport torch_geometric\nimport matplotlib.pyplot as plt\nfrom torch_geometric.loader import ClusterLoader, ClusterData\n\nfrom graph_model import SpatialModel\nfrom utils import load_data, preprocessing\n\nwarnings.filterwarnings(\"ignore\")\n\ndef batch_dataloader(dfs, pca_dims=500, k_graph=30, edge_weight=True, num_parts=128, batch_size=32, seed=1234):\n    torch.manual_seed(seed)\n    adata = load_data(dfs)\n    print('Data: %d cells × %d genes.' % (adata.shape[0], adata.shape[1]))\n    adata = preprocessing(adata,\n                          filter_mt=False,\n                          norm_and_log=True,\n                          z_score=True)\n\n    if sp.issparse(adata.X):\n        adata.X = adata.X.toarray()\n    gene_tensor = torch.Tensor(adata.X)\n    u, s, v = torch.pca_lowrank(gene_tensor, q=pca_dims)\n    gene_tensor = torch.matmul(gene_tensor, v)\n    adata.obsm[\"X_pca\"] = gene_tensor.numpy()\n\n    cell_coo = torch.Tensor(adata.obsm[\"spatial\"])\n\n    data = torch_geometric.data.Data(x=gene_tensor, pos=cell_coo)\n    data = torch_geometric.transforms.KNNGraph(k=k_graph, loop=False)(data)\n\n    # make distance as edge weights.\n    if edge_weight:\n        data = torch_geometric.transforms.Distance()(data)\n        data.edge_weight = 1 - data.edge_attr[:, 0]\n    else:\n        data.edge_weight = torch.ones(data.edge_index.size(1))\n    \n    data.idx = torch.arange(adata.shape[0])\n\n    cluster_data = ClusterData(data, num_parts=num_parts)\n    train_loader = ClusterLoader(cluster_data, batch_size=batch_size, shuffle=True)\n\n    return data, adata, train_loader\n\n\nclass Trainer:\n    def __init__(self, input_dims):\n        self.input_dims = input_dims\n        self.device = torch.device('cpu')\n\n        gae_dims = [32, 8]\n        dae_dims = [100, 20]\n        self.model = SpatialModel(input_dims=self.input_dims,\n                                  gae_dims=gae_dims,\n                                  dae_dims=dae_dims).to(self.device)\n\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01, weight_decay=1e-4)\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=1.0)\n        self.scaler = torch.cuda.amp.GradScaler()\n\n    def load_checkpoint(self, path):\n        checkpoint = torch.load(path)\n        self.model.load_state_dict(checkpoint['model'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n\n    def save_checkpoint(self, path):\n        state = {'model': self.model.state_dict(),\n                 'optimizer': self.optimizer.state_dict()}\n        torch.save(state, path)\n\n    def train(self, train_loader, epochs=200, w_dae=1.0, w_gae=1.0):\n        self.model.train()\n        start_time = time.time()\n        for epoch in range(1, epochs + 1):\n            train_loss = 0\n            for batch, data in enumerate(train_loader, start=1):\n                data = data.to(self.device, non_blocking=True)\n                inputs = data.x\n                edge_index = data.edge_index\n                edge_weight = data.edge_weight\n                with torch.cuda.amp.autocast():\n                    feat, dae_loss, gae_loss = self.model(inputs, edge_index, edge_weight)\n                    loss = w_dae * dae_loss + w_gae * gae_loss\n                train_loss += loss.item()\n                self.optimizer.zero_grad()\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n                self.scheduler.step()\n                train_loss = train_loss / len(train_loader)\n                process_time = time.time() - start_time\n                print(\"  [ Epoch %d\\t Batch %d ] Loss: %.5f, Time: %.2f s\" % (epoch, batch, train_loss, process_time))\n\n    def inference(self, test_loader, cell_nums):\n        self.model.eval()\n        output = np.zeros((cell_nums, self.model.feat_dims))\n        for data in test_loader:\n            data = data.to(self.device)\n            idx = data.idx.detach().cpu().numpy()\n            feat, _, _ = self.model(data.x, data.edge_index, data.edge_weight)\n            output[idx] = feat.detach().cpu().numpy()\n        return output\n\n\ndef cluster_block(feat, adata, indices, save_path, n_neighbors=30, resolution=0.5):\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n\n    print('clustering ......')\n    st = time.time()\n    adata_feat = anndata.AnnData(feat[indices], obs=pd.DataFrame(index=map(str, indices)))\n    adata_feat.obsm[\"spatial\"] = adata.obsm[\"spatial\"][indices]\n    adata_feat.obsm[\"X_input\"] = adata.obsm[\"X_pca\"][indices]\n    sc.pp.neighbors(adata_feat, n_neighbors=n_neighbors)\n    sc.tl.leiden(adata_feat, resolution=resolution)\n    clusters = adata_feat.obs[\"leiden\"].tolist()\n    results = pd.DataFrame({\"id\": adata[indices].obs.index.tolist(), \"label\": clusters})\n    tag = '5'  # 初赛数据集中的tag都是5\n    results['id'] = results['id'].apply(lambda x: tag+'_'+str(x))\n    results.to_csv(osp.join(save_path, \"submit.csv\"), index=False)\n    print(\"cluster results has been saved in path: \", save_path)\n    cost_time_cluster = time.time()-st\n    print('clustering finished, cost time(s): ',cost_time_cluster)\n\n    return cost_time_cluster"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294256e8-b020-4483-920e-1d6a4db0c3f6",
   "metadata": {
    "execution_count": 3,
    "jupyter": {
     "outputs_hidden": true
    },
    "msg_id": "36033690-11ef-4d1d-9b8d-2cea66efa851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "epochs:  20\npreparing data......\nData: 87845 cells × 27754 genes.\n"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Computing METIS partitioning...\nDone!\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "preparing data finished, cost time(s):  769.4470381736755\ntraining ......\n  [ Epoch 1\t Batch 1 ] Loss: 1.32103, Time: 5.30 s\n  [ Epoch 1\t Batch 2 ] Loss: 1.56710, Time: 10.08 s\n  [ Epoch 1\t Batch 3 ] Loss: 1.64284, Time: 14.68 s\n  [ Epoch 1\t Batch 4 ] Loss: 1.53608, Time: 19.28 s\n  [ Epoch 2\t Batch 1 ] Loss: 1.20800, Time: 23.92 s\n  [ Epoch 2\t Batch 2 ] Loss: 1.43579, Time: 28.47 s\n  [ Epoch 2\t Batch 3 ] Loss: 1.47239, Time: 33.58 s\n  [ Epoch 2\t Batch 4 ] Loss: 1.47082, Time: 38.27 s\n  [ Epoch 3\t Batch 1 ] Loss: 1.07128, Time: 43.09 s\n  [ Epoch 3\t Batch 2 ] Loss: 1.48448, Time: 48.08 s\n  [ Epoch 3\t Batch 3 ] Loss: 1.37624, Time: 52.88 s\n  [ Epoch 3\t Batch 4 ] Loss: 1.47025, Time: 57.47 s\n  [ Epoch 4\t Batch 1 ] Loss: 1.02382, Time: 62.38 s\n  [ Epoch 4\t Batch 2 ] Loss: 1.39670, Time: 67.20 s\n  [ Epoch 4\t Batch 3 ] Loss: 1.48581, Time: 73.98 s\n  [ Epoch 4\t Batch 4 ] Loss: 1.42416, Time: 78.58 s\n  [ Epoch 5\t Batch 1 ] Loss: 1.10072, Time: 83.41 s\n  [ Epoch 5\t Batch 2 ] Loss: 1.26134, Time: 88.27 s\n  [ Epoch 5\t Batch 3 ] Loss: 1.40003, Time: 93.08 s\n  [ Epoch 5\t Batch 4 ] Loss: 1.51504, Time: 97.88 s\n  [ Epoch 6\t Batch 1 ] Loss: 1.08614, Time: 102.77 s\n  [ Epoch 6\t Batch 2 ] Loss: 1.26348, Time: 107.58 s\n  [ Epoch 6\t Batch 3 ] Loss: 1.42414, Time: 112.48 s\n  [ Epoch 6\t Batch 4 ] Loss: 1.47975, Time: 117.30 s\n  [ Epoch 7\t Batch 1 ] Loss: 1.11919, Time: 122.10 s\n  [ Epoch 7\t Batch 2 ] Loss: 1.38312, Time: 126.88 s\n  [ Epoch 7\t Batch 3 ] Loss: 1.43292, Time: 131.69 s\n  [ Epoch 7\t Batch 4 ] Loss: 1.34357, Time: 136.48 s\n  [ Epoch 8\t Batch 1 ] Loss: 1.04368, Time: 141.10 s\n  [ Epoch 8\t Batch 2 ] Loss: 1.34899, Time: 145.80 s\n  [ Epoch 8\t Batch 3 ] Loss: 1.41711, Time: 150.48 s\n  [ Epoch 8\t Batch 4 ] Loss: 1.42714, Time: 155.31 s\n  [ Epoch 9\t Batch 1 ] Loss: 1.01657, Time: 160.09 s\n  [ Epoch 9\t Batch 2 ] Loss: 1.41954, Time: 164.90 s\n  [ Epoch 9\t Batch 3 ] Loss: 1.40637, Time: 169.78 s\n  [ Epoch 9\t Batch 4 ] Loss: 1.40000, Time: 174.40 s\n  [ Epoch 10\t Batch 1 ] Loss: 1.02297, Time: 179.10 s\n  [ Epoch 10\t Batch 2 ] Loss: 1.30352, Time: 183.78 s\n  [ Epoch 10\t Batch 3 ] Loss: 1.55338, Time: 188.50 s\n  [ Epoch 10\t Batch 4 ] Loss: 1.35629, Time: 193.50 s\n  [ Epoch 11\t Batch 1 ] Loss: 1.14962, Time: 198.49 s\n  [ Epoch 11\t Batch 2 ] Loss: 1.34462, Time: 203.10 s\n  [ Epoch 11\t Batch 3 ] Loss: 1.36205, Time: 207.99 s\n  [ Epoch 11\t Batch 4 ] Loss: 1.36284, Time: 212.79 s\n  [ Epoch 12\t Batch 1 ] Loss: 1.03809, Time: 219.09 s\n  [ Epoch 12\t Batch 2 ] Loss: 1.35326, Time: 223.68 s\n  [ Epoch 12\t Batch 3 ] Loss: 1.34491, Time: 228.18 s\n  [ Epoch 12\t Batch 4 ] Loss: 1.44279, Time: 232.83 s\n  [ Epoch 13\t Batch 1 ] Loss: 1.09467, Time: 237.51 s\n  [ Epoch 13\t Batch 2 ] Loss: 1.30583, Time: 242.11 s\n  [ Epoch 13\t Batch 3 ] Loss: 1.35067, Time: 246.79 s\n  [ Epoch 13\t Batch 4 ] Loss: 1.42286, Time: 252.40 s\n  [ Epoch 14\t Batch 1 ] Loss: 1.17391, Time: 257.21 s\n  [ Epoch 14\t Batch 2 ] Loss: 1.29405, Time: 261.97 s\n  [ Epoch 14\t Batch 3 ] Loss: 1.37532, Time: 266.78 s\n  [ Epoch 14\t Batch 4 ] Loss: 1.34698, Time: 271.47 s\n  [ Epoch 15\t Batch 1 ] Loss: 1.00958, Time: 276.32 s\n  [ Epoch 15\t Batch 2 ] Loss: 1.25516, Time: 281.08 s\n  [ Epoch 15\t Batch 3 ] Loss: 1.44477, Time: 285.88 s\n  [ Epoch 15\t Batch 4 ] Loss: 1.43801, Time: 290.40 s\n  [ Epoch 16\t Batch 1 ] Loss: 1.16664, Time: 295.09 s\n  [ Epoch 16\t Batch 2 ] Loss: 1.38119, Time: 299.79 s\n  [ Epoch 16\t Batch 3 ] Loss: 1.34478, Time: 304.49 s\n  [ Epoch 16\t Batch 4 ] Loss: 1.29427, Time: 309.38 s\n  [ Epoch 17\t Batch 1 ] Loss: 1.10636, Time: 313.97 s\n  [ Epoch 17\t Batch 2 ] Loss: 1.27037, Time: 318.68 s\n  [ Epoch 17\t Batch 3 ] Loss: 1.32636, Time: 323.31 s\n  [ Epoch 17\t Batch 4 ] Loss: 1.43708, Time: 329.79 s\n  [ Epoch 18\t Batch 1 ] Loss: 1.04067, Time: 334.58 s\n  [ Epoch 18\t Batch 2 ] Loss: 1.31673, Time: 339.28 s\n  [ Epoch 18\t Batch 3 ] Loss: 1.50015, Time: 344.39 s\n  [ Epoch 18\t Batch 4 ] Loss: 1.30985, Time: 349.70 s\n  [ Epoch 19\t Batch 1 ] Loss: 1.04064, Time: 354.58 s\n  [ Epoch 19\t Batch 2 ] Loss: 1.29965, Time: 359.57 s\n  [ Epoch 19\t Batch 3 ] Loss: 1.40359, Time: 364.28 s\n  [ Epoch 19\t Batch 4 ] Loss: 1.38414, Time: 368.90 s\n  [ Epoch 20\t Batch 1 ] Loss: 1.03923, Time: 373.58 s\n  [ Epoch 20\t Batch 2 ] Loss: 1.29025, Time: 378.78 s\n  [ Epoch 20\t Batch 3 ] Loss: 1.36666, Time: 384.20 s\n  [ Epoch 20\t Batch 4 ] Loss: 1.41605, Time: 388.98 s\ntraining finished, cost time(s):  401.75192379951477\nclustering ......\ncluster results has been saved in path:  ../result\nclustering finished, cost time(s):  262.23443937301636\n"
    }
   ],
   "source": "epochs = 20\ndatas = [df]\nsave_path = '../result'\n\nprint('epochs: ', epochs)\n\nprint('preparing data......')\nst = time.time()\ndata, adata, train_loader = batch_dataloader(datas)\ncost_time_prepare_data = time.time()-st\nprint('preparing data finished, cost time(s): ', cost_time_prepare_data)\n\nprint('training ......')\nst = time.time()\ntrainer = Trainer(input_dims=data.num_features)\ntrainer.train(train_loader=train_loader, epochs=epochs)\nfeat = trainer.inference(train_loader, adata.shape[0])\ncost_time_train = time.time()-st\nprint('training finished, cost time(s): ',cost_time_train)\n\ncost_time_cluster = cluster_block(feat=feat, adata=adata, indices=list(range(feat.shape[0])),\n                            save_path=save_path, n_neighbors=30, resolution=0.5)\n\nparams = {\"cost_time_prepare_data\": str(cost_time_prepare_data), \n            \"cost_time_train\": str(cost_time_train), \n            \"cost_time_cluster\": str(cost_time_cluster)}\n\n\n# AI靶场平台使用否工具包，用于记录全量运行参数和评估指标。\nfrom wf_analyse.analyse import wflogger\nwflogger.log_params(params=params)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bbaaa-1f1f-4077-87d5-6273a74991d6",
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": "python-3.7.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}